Suppose that a certain source has five symbols u1, u2, u3, u4 and u5, corresponding to the probability P1=0.4, P2=0.1, P3=P4=0.2, P5=0.1. First, the symbols are queued in descending order of probability, as shown in the figure. When encoding, start from the two symbols with the smallest probability, and select one branch as 0 and the other branch as 1. Here, we choose 0 for the upper branch and 1 for the lower branch. Then merge the probabilities of the two branches that have been coded, and re-queue. Repeatedly use the above method several times until the combined probability is normalized. It can be seen from Figures (a) and (b) that although the average code lengths of the two are equal, the same symbol can have different code lengths, that is, the coding method is not unique. The reason is that when the two branches are re-queued after the probability is merged , There may be several branches with equal probability, resulting in a non-unique queuing method. In general, if the newly merged branch is arranged to the uppermost branch with equal probability, it will help shorten the code length variance, and the compiled code is closer to the equal length code. Here the coding of picture (a) is better than (b).
The code word of Huffman code (the code of each symbol) is a different prefix code word, that is, any code word will not be the front part of another code word, which enables the code words to be transmitted together without the need for the middle With the addition of an isolation symbol, as long as there is no error during transmission, the receiving end can still separate each codeword without confusion.
In practical applications, in addition to using regular cleaning to eliminate error diffusion and buffer storage to solve rate matching, the main problem is to solve the statistical matching of small symbol sets, such as the statistical matching of black (1) and white (0) fax sources. Use 0 and 1 different length runs to form an expanded symbol set source. Run length refers to the length of the same code element (such as the length or number of a continuous string of 0s or 1s in a binary code). According to the CCITT standard, it is necessary to count 2¡Á1728 kinds of run lengths (lengths), so that the storage capacity during implementation is too large. In fact, the probability of a long run is very small, so CCITT also stipulates: if l represents the length of the run, then l=64q+r. Among them, q is called the main code and r is the base code. When encoding, the run length of not less than 64 is composed of the main code and the base code. And when l is an integer multiple of 64, only the code of the main code is used, and there is no code of the base code.
The long-run main code and base code are both coded by Huffman rules, which is called modified Huffman code, and the results are available in tables. This method has been widely used in document fax machines.
Theoremedit
In variable word length coding, if the code word length is arranged in reverse order strictly according to the probability of the corresponding symbol, the average code word length is the smallest.
Now an example is used to illustrate the realization of the above theorem. Suppose the source symbols are arranged in the order of their probability of occurrence as:
U: (a1 a2 a3 a4 a5 a6 a7) [1]
0.20 0.19 0.18 0.17 0.15 0.10 0.01
Assign the two symbols a6 and a7 with the smallest probability as "1" and "0" respectively, and then add their probabilities and combine them with the original a1~a5 and reorder them into the new original:
U¡ä: (a1 a2 a3 a4 a5 a6¡ä)
0.20 0.19 0.18 0.17 0.15 0.11
After assigning "1" and "0" to a5 and a'6 respectively, add the probabilities and re-sort them by probability to get
U¡å: (0.26 0.20 0.19 0.18 0.17)...
Until the end U"": (0.61 0.39)
The specific method of Huffman coding: first queue up according to the probability of appearance, add the two smallest probabilities, and re-queue as the new probability and the remaining probability, then add the smallest two probabilities, and re-queue, Until finally it becomes 1. Each time you add, "0" and "1" are assigned to the two probabilities of the addition. When reading out, you will start with the symbol and go to the last "1", and combine the "0" and "1" encountered on the route. "1" is arranged from the lowest bit to the highest bit, which is the Huffman code of the symbol.
For example, a7 from left to right, from U to U"", the codeword is 1000;
a6 According to the route, arrange the encountered "0" and "1" in the order from the lowest bit to the highest bit, the code word is 1001...
The average bit rate obtained by Huffman coding: ¦² code length ¡Á probability of occurrence
The above example is: 0.2¡Á2+0.19¡Á2+0.18¡Á3+0.17¡Á3+0.15¡Á3+0.1¡Á4+0.01¡Á4=2.72 bit
It can be calculated that the source entropy of this example is 2.61bit, which is very close.
Typeedit
Static Huffman coding
The Huffman coding was developed by Professor Huffman in the 1950s. It uses the tree structure in the data structure to construct an optimal binary tree with the support of the Huffman algorithm. The class tree is named Huffman tree. Therefore, to be precise, Huffman coding is a form of coding constructed on the basis of Huffman tree, and it has a very wide range of applications. Then, Huffman How does Mann coding achieve data compression and decompression?
As we all know, in the computer, the storage and processing of data are all based on bytes. A Western character must be expressed by one byte, and a Chinese character must be expressed by two bytes. The encoding form in which the characters are expressed by the same number of bytes is called fixed-length encoding. Take Western as an example, for example, if we want to store the sentence in the computer: I am a teacher. It takes 15 bytes, which is 120 binary bits of data. Different from this fixed-length encoding, Huffman encoding is a variable-length encoding. It constructs the encoding with the shortest average length according to the probability of character appearance. In other words, if a character If there are more occurrences in a document, its encoding is correspondingly short. If a character appears less frequently in a document, its encoding is correspondingly longer. When encoding, the length of each codeword strictly follows the corresponding symbol When the probability of occurrence is arranged in reverse order, the average length of the code is the smallest. This is the basic principle of Huffman coding to achieve data compression. To obtain the Huffman code of a piece of data, three steps are required: First: Step: Scan the data to be encoded, and count the occurrence probability of each character in the original data. Step 2: Use the obtained probability value to create a Huffman tree. Step 3: Encode the Huffman tree and get it after encoding The codeword is stored.
Because the fixed-length encoding has used the same number of bits to ensure that the encoding of any character will not become the prefix of other encodings, this situation will only appear in variable-length encoding. To avoid this, we will A condition must be used to stipulate the long code. This condition is that in order to become a compression code, the variable-length code must be a prefix code. What is a prefix code? The so-called prefix code is that the code of any one character cannot be another character Encoding prefix.
So is Huffman coding a prefix coding? Observing the coding tree composed of a, b, c, and d, we can find that the reason why b becomes the prefix of c is because in this tree, b becomes the parent node of c From the Huffman tree, the data characters in the original document are all distributed in the leaf position of the Huffman tree, thus ensuring that the encoding of any character in the Huffman encoding cannot be another character encoding Prefix. That is to say, Huffman encoding is a kind of prefix encoding, which guarantees the accuracy of decoding during the understanding of the compression process. The decompression process of Huffman encoding is also relatively simple, that is, the encoding strictly follows the Huffman tree Translation is fine. For example, if you encounter 000, you can find I along the Huffman tree, and if you encounter 101, you can find the space along the Huffman tree, and so on, we can find all the original characters smoothly. Huffman coding is a consistent coding and has a very wide range of applications. For example, in JPEG files, Huffman coding is applied to achieve the last step of compression; today, with the vigorous development of digital TV, Huffman coding It has become the main compression method of video signals. It should be said that the emergence of Huffman coding has ended the history of entropy coding cannot achieve the shortest encoding, and it has also made Huffman coding a very important lossless coding. [2]
The biggest disadvantage of the static Huffman method is that it needs to scan the original data twice: the first pass is to count the frequency of each character in the original data, use the obtained frequency value to create a Huffman tree and save the relevant information of the tree , It is convenient to use when decompressing; the second pass encodes the original data according to the Huffman tree obtained before, and stores the encoded information. In this way, if used in network communication, it will cause a greater delay; for applications such as file compression, additional disk access will reduce the data compression speed of the algorithm. [3]
Dynamic huffman coding
Faller et al. proposed a dynamic Huffman encoding method, which encodes data based on a dynamically changing Huffman tree, that is to say, the encoding of the t+1th character is obtained based on the first t characters in the original data. The compression and decompression subroutines have the same initialization tree. After each character is processed, the compression and decompression parties use the same algorithm to modify the Huffman tree, so this method does not need to save the tree for decompression Relevant information. The time required to compress and decompress a character is proportional to the length of the character's encoding, so the process can be carried out in real time.

We proceed in two steps. In the first step, we convert the Huffman tree of the first t characters into another form of it. In the second step, we only need to simply put all the nodes on the alol path from the root to the leaf node. Add 1 to the weight and it becomes a Huffman tree with the first t+1 characters. The process is to take the leaf node a(it+1) as the initial current node, repeatedly exchange the current node with the node with the same weight with the largest number, and make the latter's parent node a new one. The current node until the root node is encountered. Taking Figure 1 as an example, node 2 does not need to be exchanged, so node 4 becomes the new current node, node 4 and node 5
Exchange, node 8 becomes the current node, and finally node 8 exchanges with node 9, and node n becomes the current node, ending the cycle. So far, the first step has been completed, and the result is shown in Figure 2. It is easy to verify that it is also a Huffman tree form of the first t characters, because the exchange is only performed between nodes of the same weight. In the second step, by adding 1 to the weight of all nodes on the path from the root to the leaf node a(it+1), the tree becomes a Huffman tree with the first t+1 characters. As shown in Figure 3. A relatively complete example of dynamic Huffman coding is shown in Figure 4.